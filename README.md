<h1 align="center" style="font-weight: bold;">Scraping Google Reviews ğŸ’»</h1>


<h2>ğŸ“¦ Tecnologias usadas:</h2>

- [Django](https://www.djangoproject.com/)
- [Django REST Framework](https://www.django-rest-framework.org/)
- [Axios](https://axios-http.com/)
- [Puppeteer](https://pptr.dev/)
- [Moment](https://momentjs.com/)
- **ServiÃ§os AWS:**
  - [AWS Lambda](https://docs.aws.amazon.com/lambda/)
  - [AWS SQS](https://docs.aws.amazon.com/sqs/)
  - [AWS RDS](https://docs.aws.amazon.com/rds/)

<h2>ğŸ”¥ IntroduÃ§Ã£o:</h2>

<h3>âš™ï¸ PrÃ©-requisitos:</h3>

VocÃª precisa ter instalado na sua mÃ¡quina as seguintes tecnologias nas exatas versÃµes apontadas abaixo:
- Serverless Framework V4
- Node 20.x
- Python 3.12

*Obs.: Se quiser, vocÃª pode usar o Node ou o Python em versÃµes diferentes, mas, lembre-se de ajustar a versÃ£o no serverless.yml dos microserviÃ§os. AlÃ©m disto, dependendo de qual versÃ£o serÃ¡ utilizada, avalie se nÃ£o precisa alterar algo no cÃ³digo.

<h3>ğŸ”¨ Guia de instalaÃ§Ã£o:</h3>

<h4>Passo 1: Clone esse repositÃ³rio</h4>

```bash
git clone git@github.com:EmanuelLacerda/scraping-google-reviews.git
```

<h4>Passo 2: Acesse a pasta do respositÃ³rio</h4>

```bash
cd scraping-google-reviews
```

<h4>Passo 3: Instale as dependÃªncias</h4>

Neste repositÃ³rio, hÃ¡ 3 diretÃ³rios, sendo uma lambda function cada um deles. Devido serem lambda function, eles possuem dependÃªncias prÃ³prias. EntÃ£o, vocÃª precisa instalar as dependÃªncias individualmente. Abaixo segue como fazer para o repositÃ³rio "scheduler". Repita o mesmo para os outros dois.

**1Â° Acesse o diretÃ³rio "scheduler":**

```bash
cd scheduler
```

**2Â° Execute o "npm install":**

```bash
npm install
```

**3Â° Volte para a raiz do projeto:**

```bash
cd ..
```

<h4>Passo 04: FaÃ§a o deploy da Lambda Function "Server"</h4>

**1Â° Crie o server/.env a partir do server/.env.examples:**

Ao acessar server/.env.examples vocÃª verÃ¡ o seguinte conteÃºdo:
```
STATIC_FILES_BUCKET_NAME=
AWS_REGION_NAME=us-east-1
DB_NAME=
DB_USER=postgres
DB_PASSWORD=
DB_HOST=
DB_PORT=5432
```

Em "STATIC_FILES_BUCKET_NAME", vocÃª deve colocar o nome do bucket do AWS S3 em que ficarÃ¡ os static files desta Lambda Function. Exceto por "DB_HOST", no restante das variÃ¡veis de ambiente que nÃ£o tem valor, coloque as informaÃ§Ãµes da InstÃ¢ncia PostgreSQL do AWS RDS que utilizada pelo servidor.

**2Â° Acesse o diretÃ³rio "server":**

```bash
cd server
```

**3Â° FaÃ§a o deploy para a AWS:**


```bash
serverless deploy
```

**4Â° Execute o comando `collectstatic` do Django remotamente via `wsgi manage`:**

```bash
serverless wsgi manage --command "collectstatic --noinput"
```

**5Â° Copie na variÃ¡vel "DB_HOST" em server/.env a url do host da InstÃ¢ncia do PostgreSQL criada no deploy:**

**6Â° FaÃ§a o deploy para a AWS:**

```bash
serverless deploy
```

**7Â° Execute as migrations:**

```bash
serverless  wsgi manage --command "migrate"
```

**8Â° Saia do diretÃ³rio "server":**

```bash
cd ..
```

<h4>Passo 04: FaÃ§a o deploy da Lambda Function "Scheduler"</h4>

**1Â° Crie o scheduler/.env a partir do scheduler/.env.examples:**

Ao acessar scheduler/.env.examples vocÃª verÃ¡ o seguinte conteÃºdo:
```
BASE_URL_API_V1=
SQS_QUEUE_URL=
SQS_NAME=
```

Em "SQS_NAME", vocÃª deve colocar o nome da fila FIFO do AWS SQS para qual serÃ£o enviados o id dos business que devem passar pelo scraping. AlÃ©m disto, vocÃª deve colocar em "BASE_URL_API_V1" a URL da API da Lambda Server adicionando "/api/v1/" no final dela. Portanto, o valor de "BASE_URL_API_V1" deve ser algo como "https://lo8963hdj.execute-api.us-east-1.amazonaws.com/stg/api/v1/".

**2Â° Acesse o diretÃ³rio "scheduler":**

```bash
cd scheduler
```

**3Â° Se necessÃ¡rio, altere a periodicidade da execuÃ§Ã£o da lambda function Scheduler:**

Esta lambda function Ã© acionada a cada 10 minutos. Caso vocÃª precise que o acionamento dela ocorra em uma periodicidade diferente, ajuste isto antes de fazer o deploy.


**4Â° FaÃ§a o deploy para a AWS:**


```bash
serverless deploy
```

**5Â° Copie na variÃ¡vel "SQS_QUEUE_URL" em scheduler/.env a url da fila FIFO criada no deploy:**

<h4>Passo 04: FaÃ§a o deploy da Lambda Function "Scraper"</h4>

**1Â° Crie o scraper/.env a partir do scraper/.env.examples:**

Ao acessar scraper/.env.examples vocÃª verÃ¡ o seguinte conteÃºdo:
```
BASE_URL_API_V1=
SQS_ARN=
```

Em "SQS_ARN", vocÃª deve colocar o ARN da fila FIFO do AWS SQS para qual serÃ£o enviados o id dos business que devem passar pelo scraping. AlÃ©m disto, vocÃª deve colocar em "BASE_URL_API_V1" a URL da API da Lambda Server adicionando "/api/v1/" no final dela. Portanto, o valor de "BASE_URL_API_V1" deve ser algo como "https://lo8963hdj.execute-api.us-east-1.amazonaws.com/stg/api/v1/".

**2Â° Acesse o diretÃ³rio "scraper":**

```bash
cd scraper
```

**3Â° Se necessÃ¡rio, altere o timeout da lambda function Scheduler:**

Esta lambda function tem um timeout de 5 minutos. Caso vocÃª precise que o timeout seja diferente, ajuste isto antes de fazer o deploy.


**4Â° FaÃ§a o deploy para a AWS:**


```bash
serverless deploy
```


























